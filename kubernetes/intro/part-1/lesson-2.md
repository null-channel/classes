# Lesson 2: History

To know better how to use technology, I find it can be useful to start with a rough history of a technology, especially when that history goes as far back as 1979 when chroot was created. Now, you might wonder what chroot has to do with kubernetes. Well if you don't know. go ahead and watch my introduction to contaienrs to better understand the relationship there. My point here is that since BEFORE 1979, when we used punch cards to program computers and I was not even born, we have consistently looked for the ability to isolate processes and filesystems. In the containers class we learned that this could help us with reproducible builds, but reproducible builds is not the end.

So lets talk about the history of kubernetes.

Now there are other technologies that work to isolate and secure workloads, virtual machines are a good example of this and have their own history. To understand kubernetes, and some of the things it does; it's important to understand why it uses what it does, mostly containers. Kubernetes started as a project Called Borg at google, the problem they where looking to solve was scheduling these isolated containers. You see, containers solved two main problems for them, first they created immutable infrastructure, meaning they could stand up a new container without needing to configure the machine at all, second containers are much more light weight then VM's meaning there was much less wasted space. Why is this wasted space a big deal with VM's?

When talking about isolation nothing beats full virtualization, so why use containers? well, if full isolation is not needed, you can pack a lot more containers on a single node (node is the equivalent of a physical or vm machine) and waste fewer resources because you don't waste memory and storage by packing a kernel into each container. All the containers run on the same host OS, using the host OS's linux kernel. This means you can pack only what you need into the container making them only megabytes. 

A key note here, remember, Immutable infrastructure in this case is not talking about the nodes (in most cases) that your container is running on. It's mearly talking about your container. I highly suggest that people don't think of containers as a VM, they are a issolated process restricted by the OS to only be able to use what they need. And most of the time are packed in their own immutable filesystem with only the tools and dependencies they need.

containers made it easy to start a new container making it really easy to scale your application horizontally as each container contained a read only filesystem containing all the dependencies and tools needed to run said container letting google scale their applications as much as they wanted, Not only for scaling but spinning up new services as well as updating to new versions!

But in this lies the problem, trying to schedule a lot of small containers across multiple datacenters gets hard, you can have hundreds of thousands of containers on thousands of physical machines (called nodes in kubernetes), not only this but if you need to scale this for demand, you would need to find new servers, or old servers with extra capacity to run it on. And this is a logistical nightmare as you would need to know the requirements for your app, what it needs to run as well as what it should not run on. because you may not want it to run on the same machine as another instances of it'self for better redundancy. To make things worse, what if you have a new version of your application? not only would you need to find every single node running your app, you would need to role the update out to it in a controlled manner!

Now, kubernetes is not the first, nor is it the only and I sure hope it's not the last. But it is the one we are learning! So lets dig into a little more of the history as it evolved.

So in the early to late 2000 (I honestly don't know exactly when and if someone from google would like to let me know I will re-record this video) Project Borg was created to do just this, Manage a group of machines and what processes they where running. Now this predated linux control groups (or most commonly called cgroups) that google later in 2008 introduced into the linux kernel and was strongly based off the work they did with Borg. Borg had a few issues though, one of witch was the fact that the "control plane" or "borg brain" was a monolith itself and did not/could not scale out. So later came the less talked about but functionally better and more direct comparison Omega. 

Omega fixed a lot of the borg issues and resembles a lot of the kubernetes architecture of today with one major trade off. Kubernetes is "safer" meaning it was designed for developers to be used by people that where not Omega experts. Omega was an internal tool not readily used by outside teams and kubernetes was designed for this. 

For example, Facebook has a project called tupperware, that does some of the same things, and it's probably better, For facebook. Meaning it's designed ot run their stuff their way. But if you would try to use it, you probably would not be able to even run the project! Ok, so there are other projects that are even opensource, like Meso that you could checkout or the closest competetor, but not really opensource is Nomad, all of these projects try to schedule immutable infrastructure/workloads at scale. This means that this is the problem they solve. So when you look to see if kubernetes is going to solve your problem, think if that is the problem you are having, if it is not. kubernetes is probably not going to solve your problem but add more on them!

It was then in 2014 that kubernetes was released as an opensource project and in 2015 the first v1 of kubernets hit the main stream. Since then thoustands of projects have sprung up around the kubernetes project.